{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEoPGS630o0HfklDd9Xlzy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MostHumble/NewsPaperScrape/blob/main/ScrapeFromNewsPapers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 "
      ],
      "metadata": {
        "id": "njyhH40z51rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import os\n",
        "website = ''\n",
        "def collect_article_data(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    articles = []\n",
        "    main_content = soup.find(class_='td-ss-main-content')\n",
        "    for article in main_content.find_all('h3', class_='entry-title td-module-title'):\n",
        "        link = article.find('a')['href']\n",
        "        articles.append(link)\n",
        "\n",
        "    return articles\n",
        "articles = []\n",
        "\n",
        "num_pages = 0 #edit\n",
        "\n",
        "for i in range(1,num_pages):\n",
        "  time.sleep(random.uniform(0.1, 0.3))\n",
        "  url = f'{website}/page/{i}/'\n",
        "  articles_by_page = collect_article_data(url)\n",
        "  articles.extend(articles_by_page)\n",
        "\n",
        "\n",
        "def collect_paragraphs(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    \n",
        "    paragraphs = []\n",
        "    for article in soup.find_all('article'):\n",
        "        for paragraph in article.find_all('p'):\n",
        "            paragraphs.append(paragraph.get_text())\n",
        "    \n",
        "    return paragraphs\n",
        "\n",
        "folderPath = './articles'\n",
        "if not os.path.exists(folderPath):\n",
        "    os.makedirs(folderPath)\n",
        "i = 0\n",
        "for url in articles:\n",
        "    time.sleep(random.uniform(0.2, 0.4))\n",
        "    paragraphs = collect_paragraphs(url)\n",
        "    article = ''\n",
        "\n",
        "    # Concatenate paragraphs to form the article\n",
        "    for paragraph in paragraphs:\n",
        "        article += paragraph\n",
        "\n",
        "    file_path = os.path.join(folderPath, f'article_{i}.txt')\n",
        "    i += 1\n",
        "\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write(article)\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "pSrPlZtd-qoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Utq4j1O4N8P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/articles.zip /content/articles\n"
      ],
      "metadata": {
        "id": "_Bz-K5iAOp9T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}